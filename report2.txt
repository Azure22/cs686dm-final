The problem we needs to solve remains the same but we have changed our emphasis. Rather than focus on giving each individual word a sentiment score using a brand new algorithm, we choose to first build two models which classify text into two category: positive and negative. The first model is Naive Bayes, whose features are the most important words in the training data, since weâ€™ve already got a bag of words along with their score, we just simply find out the first fifty highest score, including positive and negative. Another model is quite easy, even without the training procedure, but we need to optimize the time and space complexity and compare its accuracy to Naive Bayes. 

The goal to build a whole new word-score mapping requires us to use TF-IDF as one of our feature, and also to find out in the future a way to weigh different factor in order to calculate the final result. The experiment to see the accuracy is to calculate how much the original word-score mapping is different from ours after normalization.